{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Personality prediction based on internet posts**"},{"metadata":{},"cell_type":"markdown","source":"In this notebook we will investigate data from kaggle Myers-Briggs Personality Type Dataset. Based on information contained in this collection we will try to predict personality type of author of post.\n\nMore on test and personality types can be found here: https://www.16personalities.com/personality-types\n"},{"metadata":{},"cell_type":"markdown","source":"# Buissnes understanding\n\nLiving in the internet world, we read hundreds of posts every day by authors with different views and character traits. Having data that collects posts labeled with the personality type of the user who wrote them I decided to answer the three below questions:\n\n1. What is the distribution of each personality trait among forum users?\n2. Are there any most significant word for every trait? Do extroverts write in a different way than introverts? Or maybe people with more sensitive approach are using emotional vocabulary?\n3. Can personality type be predicted based on someones post?\n\nThanks to this recruiters could get to know potential job candidates better, based on the information they have provided about themselves, e.g. on Linkedin. This could also be used on dating sites, where machine learning algorithms could select partners based on their personality, inferred from their descriptions about themselves."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# import all necessary packages\nimport re\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data understanding & Data preparation"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load and display data\npd.options.display.max_colwidth = 100\n\ndata_df = pd.read_csv(\"../input/mbti-type/mbti_1.csv\")\ndata_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if there are missing values\ndata_df.isna().mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset consists of 2 columns with 8675 records. Every record contains authors personality type and his or hers published posts. It doesn't have any empty values so we don't need to worry about dropping or imputing missing data.\n\nLet's display first 1000 characters of random record."},{"metadata":{"trusted":true},"cell_type":"code","source":"random_record = data_df.loc[42][\"type\"], data_df.loc[42][\"posts\"]\nprint(\"type: \", random_record[0], \"\\nposts: \", random_record[1][:1000])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok so before applying any modification to data we check how does the distribution of every type in dataset looks like"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_dist(data_dist, save_fig=False):\n    \"\"\"\n    Plots seaborn barplot\n    args: data_dist is a dictionary with trait name as key and it's occurences as value\n          save_fig is boolean incitacing if figure should be saved as png file\n    \"\"\"\n    fig = plt.figure(figsize=(15,5))\n    sns.barplot(data_dist.index, data_dist.values)\n    plt.ylabel('Type occurrences', fontsize=15)\n    plt.xlabel('Personality type', fontsize=15)\n    \n    if save_fig:\n        fig.savefig('distribution.png')\n        \nplot_dist(data_df[\"type\"].value_counts(normalize=True), save_fig=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well this is... interesting. It seems that we have much more introverts going throug life with strong intution (first two letters in personality type) in society. But is this accurate with official statistics? Not really, here: https://www.myersbriggs.org/my-mbti-personality-type/my-mbti-results/how-frequent-is-my-type.htm?bhcp=1 we can se that out dataset is not quite representative for whole society and because of that is strongly imbalanced.\n\nAccording to graphics presented above we should investigate what is the distribution between each singular trait in our data. That means we want to check how looks a ratio:\n* introverts to extraverts,\n* intuition to sensing,\n* feeling to thinking,\n* judging to perceiving\n\nTo do that, let's encode our labels to categorized values."},{"metadata":{"trusted":true},"cell_type":"code","source":"def encode(personality):\n    \"\"\"Given string with four uppercase letters, returns list of int values: 1 for every letter (I, N, F, P), 0 otherwise\"\"\"\n    encoder = defaultdict(int, {\"I\": 1, \"N\": 1, \"F\": 1, \"P\": 1})\n    return [encoder[letter] for letter in list(personality)]\n\nprint(encode(\"ENTP\"))\nprint(encode(\"ISFJ\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df = pd.concat([pd.DataFrame.from_records(data_df[\"type\"].apply(lambda personality: encode(personality)), columns=[\"I-E\", \"N-S\", \"F-T\", \"P-J\"]), data_df], axis=1)\ndata_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"left = np.array([data_df[\"I-E\"].sum(), data_df[\"N-S\"].sum(), data_df[\"F-T\"].sum(), data_df[\"P-J\"].sum()])\nright = np.array([len(data_df)] * 4) - left\n\ncoordinates = [3, 2, 1, 0]\n\np1 = plt.barh(coordinates, left, color=['#5fba7d'])\np2 = plt.barh(coordinates, right, left=left, color=['#d65f5f'])\n\nplt.title(\"Ratio between type indicators\")\nplt.xlabel(\"Count\")\nplt.ylabel(\"Type\")\nplt.yticks(coordinates, (\"I-E\", \"N-S\", \"F-T\", \"P-J\"))\n\nplt.show()\nplt.savefig(\"ratio.png\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ok, so we can see that distribution is nothing like provided by test authors. Reason of this incompatibility may be related to fact that introverts are much more focused on their \"inside world\". So they tended to patricipate and discuss this kind of tests more offen than more expresive colleuges. Although I have no idea why this selected group of people are so insensitive :("},{"metadata":{},"cell_type":"markdown","source":"Now we should investigate text data but right now it looks a little bit messy. It seems that every post is separated by \"|||\" sign and it has a lot of redundant signs and traits which make analisys harder. We should clean up this text.\n\nWe would like to get rid of every:\n* url\n* digits\n* non ascii characters\n* most common words in certain language\n\nAnd also:\n* lowercase our text\n* as well as lemmatize every word"},{"metadata":{"trusted":true},"cell_type":"code","source":"def lemmatize_text(text):\n    lem = WordNetLemmatizer()\n    text = text.split(\" \")\n    text = [lem.lemmatize(word) for word in text]\n    \n    return \" \".join(text)\n\ndef remove_stopwords(text, stopwords):\n    text = text.split(\" \")\n    return \" \".join([word for word in text if word not in stopwords])\n\ndef preprocess_text(text, stopwords):\n    text = re.sub(r'http\\S+', '', text)  # remove url links\n    text = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove every non ascii character and digit\n    text = text.lower()\n    text = remove_stopwords(text, stopwords)\n    text = lemmatize_text(text)\n    \n    text = re.sub(' +', ' ', text)  # remove extra whitespaces\n    return text\n\ndef simple_processing(text, types):\n    text = re.sub(r'http\\S+', '', text)  # remove url links\n    text = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove every non ascii character and digit\n    text = text.lower()\n    text = remove_stopwords(text, types)\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's see how does above function affects out text"},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_stopwords = stopwords.words(\"english\")\npersonality_types = list({type.lower() for type in set(data_df[\"type\"])})\npersonality_types.extend([pt + \"s\" for pt in personality_types]) # a lot of people are using personality type in plural form with extra s at the end, we should remove it as well\nmy_stopwords = set(eng_stopwords).union(personality_types)  # we don't want to keep personality type acronyms in text, so we add them to stopwords\n\nrnd_idx = np.random.randint(low=0, high=len(data_df)-1, size=2)\nfor idx in rnd_idx:\n    random_record = data_df.loc[idx][\"type\"], data_df.loc[idx][\"posts\"]\n    print(\"type: \", random_record[0], \"\\nposts: \", preprocess_text(random_record[1], my_stopwords), \"\\n\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It's a lot of text, but it looks much cleaner. Now we will apply this function and clean text for every row in posts column. This may took around minute or two."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df[\"preprocessed_posts\"] = data_df[\"posts\"].apply(lambda text: preprocess_text(text, my_stopwords))\ndata_df[\"posts\"] = data_df[\"posts\"].apply(lambda text: simple_processing(text, personality_types))\ndata_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can check what are the most common words between each group."},{"metadata":{"trusted":true},"cell_type":"code","source":"introverts_vocab = data_df.loc[data_df[\"I-E\"] == 1][\"preprocessed_posts\"].sum().split(\" \")\nextraverts_vocab = data_df.loc[data_df[\"I-E\"] == 0][\"preprocessed_posts\"].sum().split(\" \")\nfeelers_vocab = data_df.loc[data_df[\"F-T\"] == 1][\"preprocessed_posts\"].sum().split(\" \")\nthinkers_vocab = data_df.loc[data_df[\"F-T\"] == 0][\"preprocessed_posts\"].sum().split(\" \")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_top_n_words(corpus, n=None):\n    \"\"\"Gets n most common words from corpus\"\"\"\n    words_freq = FreqDist(corpus)\n    return words_freq.most_common(n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_most_common_words(vocabulary):\n    \"\"\"Plots most common words in vocabulary as bar plot, with descending order\"\"\"\n    top_words = get_top_n_words(vocabulary, n=20)\n    top_df = pd.DataFrame(top_words)\n    top_df.columns=[\"Word\", \"Freq\"]\n\n    sns.set(rc={'figure.figsize':(13,8)})\n    g = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df);\n    g.set_xticklabels(g.get_xticklabels(), rotation=30);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f = plt.figure(figsize=(20,10))\nf.tight_layout()\n\nf.add_subplot(4, 2, 1)\nplt.title(\"Introverts vocab\")\nshow_most_common_words(introverts_vocab)\n\nf.add_subplot(4, 2, 2)\nplt.title(\"Extraverts vocab\")\nshow_most_common_words(extraverts_vocab)\n\nplt.subplots_adjust(left=None, bottom=None, right=None, top=3, wspace=None, hspace=None)\n\nf.add_subplot(4, 2, 3)\nplt.title(\"Feelers vocab\")\nshow_most_common_words(feelers_vocab)\n\nf.add_subplot(4, 2, 4)\nplt.title(\"Thinkers vocab\")\nshow_most_common_words(thinkers_vocab)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So we can see that there is no clearly more popular words for each personality trait. We could remove ones which are most popular among each group, but maybe there is better way. Let's change text representation to numbers."},{"metadata":{"trusted":true},"cell_type":"code","source":"cv = CountVectorizer(max_df=0.90, max_features=10000, ngram_range=(1,1))\ntfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n\nX = cv.fit_transform(data_df[\"preprocessed_posts\"])\nfeature_names = cv.get_feature_names()\n\ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_vector = tfidf_transformer.fit_transform(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tfidf_vector.todense())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now when we have our data ready to use with machine learning algorighm we can train our model and see how it's behaves"},{"metadata":{},"cell_type":"markdown","source":"# Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"l_svc = LinearSVC(C=100, verbose=3, max_iter=10000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = tfidf_vector\ny = data_df[\"I-E\"]\n\n# beacuse of highly imbalanced dataset, we need to use stratify option, to make sure that each class representation will be accurate\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y, shuffle=True)\n\n# check if data is properly splitted\nprint(X_train.shape)\nprint(X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"l_svc.fit(X_train, y_train)\ny_pred = l_svc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Evaluation"},{"metadata":{},"cell_type":"markdown","source":"Cell below will plot colorful confusion matrix for results predicted by classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"types_list = [\"extraverts\", \"introverts\"]\ncm = confusion_matrix(y_pred, y_test)\n\ndf_cm = pd.DataFrame(cm, types_list, types_list)\nfig = plt.figure(figsize = (13,10))\n\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}, fmt='g')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"More detailed report"},{"metadata":{"trusted":true},"cell_type":"code","source":"report = classification_report(y_pred, y_test)\nprint(report)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_coefficients(classifier, feature_names, n_features=20):\n    \"\"\"Plots a bar plot with feature importances for provided classifier. To work classifier must have coef_ attribute\"\"\"\n    coef = classifier.coef_.ravel()\n    feature_names = np.array(feature_names)\n    \n    top_pos_coefficients = np.argsort(coef)[-n_features:]\n    top_neg_coefficients = np.argsort(coef)[:n_features]\n    top_coefficients = np.hstack([top_neg_coefficients, top_pos_coefficients])\n    \n    # create plot\n    plt.figure(figsize=(15, 5))\n    colors = [\"red\" if c < 0 else \"blue\" for c in coef[top_coefficients]]\n    plt.bar(np.arange(2 * n_features), coef[top_coefficients], color=colors)\n    plt.xticks(np.arange(1, 1 + 2 * n_features), feature_names[top_coefficients], rotation=60, ha=\"right\")\n    plt.show()\n\nplot_coefficients(l_svc, cv.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_importances(coef, features_names, n_top=-1):\n    coef, features_names = zip(*sorted(list(zip(coef, features_names))))\n\n    # Show all features\n    if n_top == -1:\n        n_top = len(features_names)\n\n    plt.barh(range(n_top), coef[::-1][0:n_top], align='center')\n    plt.yticks(range(n_top), features_names[::-1][0:n_top])\n    plt.show()\n\nfeature_importances(abs(l_svc.coef_[0]), cv.get_feature_names(), n_top=20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will train four different binary classifiers and print classification result for every one of them"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = tfidf_vector\nfor type_trait in (\"I-E\", \"N-S\", \"F-T\", \"P-J\"):\n    y = data_df[type_trait]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=77, stratify=y, shuffle=True)\n    \n    print(f\"Fitting {type_trait}\")\n    l_svc.fit(X_train, y_train)\n    y_pred = l_svc.predict(X_test)\n    report = classification_report(y_pred, y_test)\n    print(type_trait, \" report\\n\", report)\n    plot_coefficients(l_svc, cv.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_results(values, names):\n    \"\"\"Visualize results by providing score values and classes names\"\"\"\n    values = [value * 100 for value in values]\n    values, names = zip(*sorted(list(zip(values, names))))\n    \n    plt.title(\"Prediction results\")\n    plt.xlabel(\"F1-Score [%]\")\n    plt.ylabel(\"Type\")\n    plt.barh(range(4), values[::-1][:], align='center')\n    plt.yticks(range(4), names[::-1][:])\n    plt.show()\n    \nshow_results([0.65, 0.61, 0.75, 0.61], [\"I-E\", \"N-S\", \"F-T\", \"P-J\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}